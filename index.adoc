= It takes three pixels to make the seeing blind 
:imagesdir: images
:icons: font
:date: July 11, 2019
:my_name: Peter Steinbach
:my_email: steinbach@extern.mpi-cbg.de
:my_twitter: psteinb_
:my_github: psteinb
:revealjs_slideNumber: true
:revealjs_center: true
:revealjs_BackgroundVertical: null
:revealjs_width: 1920
:revealjs_hash: true
:revealjs_margin: .05
:revealjs_plugin_pdf: enabled #you run your presentation in a browser with ?print-pdf at the end of the URL, you can then use the default print function to print the slide deck into a PDF document.
:customcss: custom.css
:source-highlighter: highlightjs
:stem:

mailto:{my_email}[{my_name}] (https://www.mpi-cbg.de[MPI CBG], https://www.scionics.de[Scionics]) +
Institute Seminar {date} +
Institute for Medical Informatics and Biometry, Dresden, Germany +
https://twitter.com/{my_twitter}[icon:twitter[] psteinb_] https://github.com/{my_github}[icon:github[] psteinb] + 

== Deep Learning

=== in one slide
:stem: latexmath

// based on https://indico.mpi-cbg.de/event/118/contributions/444/attachments/166/243/ml-intro.pdf
Given:

[asciimath] 
++++ 
\vec{y} = f(\vec{x}, \theta), L( \vec{y}^{true}, f(\vec{x}, \theta))
++++

[asciimath]
++++ 
\theta \approx argmin_{\theta} L( \vec{y}^{true}, f(\vec{x}, \theta)
++++

[.XS-text]#&nbsp;#

Weight Update Rule:

[asciimath] 
++++ 
\theta_{i+1} = \theta_{i} + \eta \nabla_{\theta}L( \vec{y}^{true}, f(\vec{x}, \theta_i))
++++

(stem:[\nabla_{\theta}L] estimated by https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Extensions_and_variants[mini-batched stochastic gradient descent])

=== Supervised Deep Learning Classifiers

image:dl-process.png[width=75%]

From <<MapRBlog>>

[.notes]
--
* classification (accuracy better than human)
* large training/test set
* data set = good balance of classes
--


=== History: https://en.wikipedia.org/wiki/ImageNet[ImageNet]

[cols="2*^.^",width=90%,frame=none,grid=none]
|===
a|
[.L-text]
* curated database of "images" and labels
* 15M images in 21k synonym classes

* 2017 (last): 2.25% classification error

a|
image:imagenet_error_rate_history.svg[width=70%,align=right]

|===


[.notes]
--
* Imagenet = curated DB of web links to images and labels
* 15M images in 21k synonym classes
* 2017 LOC: 2.25% classification error
* classification/localization challenges since 2010
--

=== AI is Here!

image:ai_is_here.jpg[]

[.notes]
--
* dangerous over-generalisation
--

== Introducing Adversarial Examples

icon:question-circle[5x]

=== What are Adversarial Examples?

image:advex-explained-1-nogradient.png[width=90%]

From <<Goodfellow14>> (based on <<Szegedy14>>)

=== Fooling by printing

image:advex-explained-2.png[width=90%]

From <<Kurakin16>>

=== Fooling by backdoors

image:badnet-without-caption.png[width=50%]

From <<Gu17>>

=== Consequences!

- trust in any DL classifier
- security of autonomous driving
- security of voice recognition systems (Siri, Alexa, ...)
- scientific use of DL?

WARNING: [.XL-text]#Affects any deployed ML system!#

[.notes]
--
* how about medical imaging systems?
--

== Adversarial Attacks Against Medical Deep Learning Systems

=== Doctor's Appointment 

icon:user-md[5x]

=== Measurement

icon:heartbeat[5x]

=== Diagnosis

icon:medkit[5x] &nbsp; or &nbsp; icon:heart[5x]

=== Adversarial Attacks Against Medical Deep Learning Systems

[.XL-text]#Samuel G. Finlayson, Hyung Won Chung, Isaac S. Kohane, Andrew L. Beam (HMS+Harvard+MIT)# +

Science, 22 Mar 2019, Vol. 363, Issue 6433, pp. 1287-1289, https://doi.org/10.1126/science.aaw4399[10.1126/science.aaw4399]

[quote]
____
The prospect of improving healthcare and medicine with the use
of deep learning is truly exciting. [...]
it seems inevitable that medical deep learning algorithms will become entrenched in the already multi-billion dollar medical information technology industry.
____

[NOTE.speaker]
--
- published in Science
- reproduced it
--

[.XL-text]
=== Can the Conclusion be reproduced?

[quote, Finlayson et al., "Adversarial Attacks Against Medical Deep Learning Systems"]
____
[.XL-text]#In this work, we have outlined the systemic and technological reasons that cause adversarial examples to pose a disproportion-
ately large threat in the medical domain, and provided examples of how such attacks may be executed.#
____


=== 3 open datasets

- https://www.kaggle.com/c/diabetic-retinopathy-detection/data[Diabetic Retinopathy Detection]
- http://academictorrents.com/details/557481faacd824c83fbf57dcf7b6da9383b3235a[X-ray Dataset of 14 Common Thorax Disease]
- https://www.isic-archive.com/#!/topWithHeader/onlyHeaderTop/gallery[skin cancer dataset] (omitted from this presentation)

=== https://www.kaggle.com/c/diabetic-retinopathy-detection/data[Diabetic Retinopathy] (DR)

[cols=">.^35,<.^65"]
|===
a|
image:dr-original.png[width=80%,align=right]

a|
* https://www.kaggle.com/c/diabetic-retinopathy-detection/data[kaggle dataset]: https://en.wikipedia.org/wiki/Ophthalmoscopy[fundus photographs]
* Diabetic retinopathy is the leading cause of blindness in the working-age population of the developed world.
* Clinicians can identify DR by the presence of lesions (vascular abnormalities due to disease)
* 88702 images, max. 4752x3168 RGB jpegs
* 5 labels (severity of DR)
|===



=== http://academictorrents.com/details/557481faacd824c83fbf57dcf7b6da9383b3235a[ChestX-ray8]

[cols=">.^35,<.^65"]
|===
a|
image:cxr-original.png[width=80%,align=right]

a|
* Classification and Localization of Common Thorax Diseases
* 112,120 images
* 1024x1024 8-bit greyscale
* 14 labels, appearing non-exclusively

|===


=== Code

https://github.com/sgfin/adversarial-medicine[github.com/sgfin/adversarial-medicine]

[%step]
- notebooks reproduce all figures based on downloaded weights
- python script to retrain two architectures (https://keras.io/applications/#resnet[Resnet50], https://keras.io/applications/#inceptionv3[InceptionV3])


== From reproduction to critique

=== Claim

[quote, Finlayson et al.,"Adversarial Attacks Against Medical Deep Learning Systems"]
____
[.XL-text]#For each of our representative medical deep learning classifiers, both white and black box attacks were highly successful.#
____


=== Taxonomy based on <<Biggio18>>

[cols="4*",options="header"]
|===
|type
|white box
|grey box
|black box

|adversary knows
|all of
|subset of
|only queries

a| &nbsp;
a| 
* dataset
* feature set
* learning alg + loss
* trained weights
a| 
* [.graytext]#dataset#
* feature set
* learning alg + loss
* [.graytext]#trained weights#
a| 
* [.graytext]#dataset#
* [.graytext]#feature set#
* [.graytext]#learning alg + loss#
* [.graytext]#trained weights#
|===

WARNING: [.XL-text]#Boundaries not clear cut!#


=== White Box Attacks?
:stem: latexmath

Given:

[asciimath] 
++++ 
\vec{y} = f(\vec{x}, \theta), L( \vec{y}^{true}, f(\vec{x}, \theta))
++++

Weight Update Rule:

[asciimath] 
++++ 
\theta_{i+1} = \theta_{i} + \eta \nabla_{\theta}L( \vec{y}^{true}, f(\vec{x}, \theta_i))
++++


=== Projected Gradient Descent Attack


[asciimath] 
++++ 
x^{t+1} = Clip_{x,S} (x^{t} + \epsilon \sign(\nabla_x L(\theta,x,y)))
++++

[%step]
* iterate until stem:[x^{t+1}] is missclassified
* For a given set of allowed variations stem:[S]
* PGD <<Madry17>>,<<Kurakin1611>> is a White Box Attack!

[.notes]
--
* Using optimisation to counter optimisation
--




=== Black-Box Attacks assume knowledge

[quote]
____
[...] black box attacks were performed by crafting the attack against an independently-trained model with the same architecture and then transferring the resultant adversarial examples to the victim.
____

=== My Motivation

icon:question-circle[5x]

How about real black-box attacks?

=== https://github.com/bethgelab/foolbox[Foolbox]

[source,python,align="center"]
----
import foolbox
import keras
import numpy as np

keras_model = ...#load weights

fmodel = foolbox.models.KerasModel(keras_model, bounds=(0, 255),
				   preprocessing=preprocessing)

image, label = foolbox.utils.imagenet_example()

attack = foolbox.attacks.LinfinityBasicIterativeAttack(fmodel)
adversarial = attack(image[:, :, ::-1], label)
----

* offers a variety of white and black box attacks
* simple API can wrap models from TensorFlow, PyTorch, Theano, Keras, Lasagne, MXNet

=== Pure Black-box on Diabetic Retinopathy

[cols="^.^,^.^,^.^",width=100%,frame=none,grid=none]
|===
a|
[.text-center]
.Original
image:dr-original.png[width=80%]
a|
[.text-center]
.Adversarial
image:dr-adversarial.png[width=80%]
a|
[.text-center]
.Difference*20
image:dr-diff.png[width=80%]
|===

[.L-text,source,python]
----
attack = foolbox.attacks.SaltAndPepperNoiseAttack(fmodel)
----

[.notes]
--
* real world: shot noise in CCM chip
--

=== Easy! But why?

[cols="2*",width=100%,frame=none,grid=none]
|===
a|
[.text-center]
.Dataset yields Class Imbalance
image:dr_class_overview.svg[width=80%]

a|
[source,python]
----
train_generator = train_datagen.flow_from_directory(
            'images/train',
            target_size=(224, 224),
            batch_size=batch_size,
            class_mode='categorical',
            shuffle=True)
----

* Input image sizes range  [2500-4000]x[2000-2500]
* before training, Finlayson et al https://github.com/sgfin/adversarial-medicine/blob/1524f3d03436d0c71b75cd2d8fd2225d0fd2e36b/train_models/train_model.py#L52[resized to 224x224]!

&nbsp;

[quote, Dresden Eye Doctor, Private Discussion, role=text-right]
____
[.XL-text]#DR pathologies are very fine structures! I wouldn't trust a machine to do it.#
____

|===

[.notes]
--
* inbalanced class frequencies in training dataset
* real world: DR discovered in very fine structured elements
* subject to further study
--

=== DR Pathologies

[cols="2*^.^",frame=none,grid=none]
|===
a|
[.text-center]
.Healthy
image:dr-10-left-level0_800px.jpg[width=80%]
a|
[.text-center]
.Malignant
image:dr-16-left-level4_800px.jpg[width=80%]
|===


=== Chest X-Ray Dataset?


[cols="2*^.^",frame=none,grid=none,width=80%]
|===
a|
[.text-center]
&nbsp; +
&nbsp; +
No simple attack worked: +

* `SaltAndPepperNoiseAttack`
* `GaussianBlurAttack`
* `AdditiveUniformNoiseAttack`
* `ContrastReductionAttack`
a|
[.text-center]
&nbsp; +
&nbsp; +
image:cxr-original.png[width=50%,align=center]
|===

=== Decision Boundary Attack

image:1712.04248_fig7.png[width=80%]

Decision-Based Adversarial Attacks: Reliable Attacks Against Black-Box Machine Learning Models. +
Wieland Brendel, Jonas Rauber and Matthias Bethge +
arXiv preprint https://arxiv.org/abs/1712.04248[arXiv:1712.04248] (2017)


=== Pure Black-box Attacks on Chest X-Ray

[cols="^.<,^.<,^.<",width=100%,frame=none,grid=none]
|===
a| 
[.text-center]
.Original
image:cxr-original.png[width=80%]
`predict(label) = healthy`
a|
[.text-center]
.Adversarial Template
image:cxr-adversarial-template.png[width=80%]
`predict(label) = malignant`
a|
[.text-center]
.Adversarial
image:cxr-adversarial.png[width=80%]
`predict(label) = malignant`
|===

[.notes]
--
* model's predict function was called ~5000 times
--

=== Where was the Chest X-Ray tricked?

[cols="^.<,^.<,^.<",width=100%,frame=none,grid=none]
|===
a| 
[.text-center]
.Original
image:cxr-original.png[width=80%]
`predict(label) = healthy`
a|
[.text-center]
.Adversarial
image:cxr-adversarial.png[width=80%]
`predict(label) = malignant`
a|
[.text-center]
.Difference
image:cxr-diff.png[width=80%]
|===


== Adversarial Examples are Features

=== Adversarial Examples are https://arxiv.org/abs/1712.03141[known] <<Biggio18>>

image:1712.03141_fig8.png[]


=== Adversarial Training <<Madry2017>>

Weight Update Rule:

[asciimath] 
++++ 
\theta_{i+1} = \theta_{i} + \eta \nabla_{\theta}L( \vec{y}^{true}, f(\vec{x}, \theta_i))
++++

&nbsp;

Weight Update Rule with adversary:

[asciimath] 
++++ 
\theta_{i+1} = \theta_{i} + \eta \nabla_{\theta}L( \vec{y}^{true}, f(\vec{x} + \vec{\delta}, \theta_i))
++++

[.notes]
--
* make adversary part of the training
* shown to make models more robust to attacks
* much longer training times
--

=== Is Deep Learning Training bugged?

icon:question-circle[5x]

=== Not necessarily, it's also http://gradientscience.org/adv/[the data] <<Ilyas2019>>

[cols="2*^.^",frame=none,grid=none,width=100%]
|===
a|
image:gradientscience_adv_robust_nonrobust.png[width=80%,align=center]

a|
image:gradientscience_adv_training_results.png[width=100%,frame=none]
|===


== Summary

[role=L-text]
=== Adversarial Attacks Against Medical Deep Learning Systems

[%step]
// * robustness of DL classifiers as diagnostic systems
* successfully achieved reproducibility of paper results (praise to the authors)
* illustrated classifiers and attacks appear remote to reality
* portions of the paper contradictory in the details

[role=XL-text]
=== AI and the Bigger Picture

[%step]
* end-to-end DL may not produce machine intelligence (data intelligence?)
* expectations on DL governed by premature generalisation
* if data is all you got, better treat it carefully!
* adversarial examples
** features of data implanted into model
** tool to probe accuracy or resilience

=== DL expected to lay golden eggs

++++
<iframe width="1120" height="630" src="https://www.youtube.com/embed/jV5xsOfQFOc" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
++++

== Appendix

=== Data Access

* Diabetic Retinopathy Dataset
** 83 GB shared through kaggle

* X-ray Dataset
** 42 GB shared through science torrent 

IMPORTANT: Very Good!

[.notes]
--
* download of datasets was somewhat easy
* kaggle required to create an account
* omitted dataset used custom formatted REST API without front-end library :/
--

=== Code Quality Comments

[%step]
- code was executable once dropbox weights downloaded
- notebooks work from top to bottom
- code largely undocumented
- paper reports Resnet50 results, weights are for InceptionV3
- readme reports similar results for either


include::references.adoc[]

